### Description:

Hierarchical Clustering groups data points into clusters based on a hierarchy of merges or splits. Unlike K-Means, it doesn't require specifying the number of clusters beforehand. In this project, weâ€™ll use Agglomerative Clustering and visualize the cluster hierarchy with a dendrogram, using a synthetic 2D dataset.

- Generates synthetic data with 3 natural clusters
- Visualizes the dendrogram to show merging hierarchy
- Applies agglomerative clustering using Ward linkage
- Visualizes the final cluster assignments

## Hierarchical Clustering with Dendrogram and Agglomerative Clustering

This example demonstrates hierarchical clustering using synthetic 2D data generated by `make_blobs`. The clustering process is visualized using both a dendrogram and the results of applying `AgglomerativeClustering`.

### Code Breakdown

#### 1. Import Required Libraries

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
```

* **numpy**: For numerical operations.
* **matplotlib.pyplot**: For visualizing data.
* **sklearn.datasets.make\_blobs**: To generate sample 2D clustered data.
* **AgglomerativeClustering**: Performs hierarchical clustering.
* **dendrogram, linkage**: For visualizing hierarchical relationships between data points.

#### 2. Generate Synthetic 2D Data

```python
X, _ = make_blobs(n_samples=100, centers=3, cluster_std=1.2, random_state=42)
```

* Creates a dataset with 100 points, divided into 3 clusters, with some spread controlled by `cluster_std`.

#### 3. Visualize the Raw Data

```python
plt.figure(figsize=(6, 4))
plt.scatter(X[:, 0], X[:, 1], s=50, color='gray')
plt.title("Raw Data Points")
plt.grid(True)
plt.tight_layout()
plt.show()
```

* Plots the raw, unclustered data points.

#### 4. Create and Visualize the Dendrogram

```python
linked = linkage(X, method='ward')

plt.figure(figsize=(10, 5))
dendrogram(linked, truncate_mode='lastp', p=20, leaf_rotation=45., leaf_font_size=12.)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Sample Index or Cluster Size")
plt.ylabel("Distance")
plt.tight_layout()
plt.show()
```

* **`linkage`**: Computes a linkage matrix using Ward's method, which minimizes intra-cluster variance.
* **`dendrogram`**: Plots a hierarchical tree diagram representing merges at various distances.
* **`truncate_mode='lastp'`**: Truncates the tree to show only the last `p` merged clusters.

#### 5. Apply Agglomerative Clustering

```python
model = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')
labels = model.fit_predict(X)
```

* Applies agglomerative clustering using Ward linkage.
* **`n_clusters=3`**: Specifies the desired number of clusters.
* **`affinity='euclidean'`**: Distance metric used for clustering.

#### 6. Visualize the Clustered Data

```python
plt.figure(figsize=(6, 4))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow', s=50)
plt.title("Clusters from Agglomerative Clustering")
plt.grid(True)
plt.tight_layout()
plt.show()
```

* Visualizes the final clusters identified by the model with color-coded labels.

---

This process helps understand how hierarchical clustering groups data and how a dendrogram visually represents this grouping. The dendrogram can guide the selection of the number of clusters before applying `AgglomerativeClustering`.
